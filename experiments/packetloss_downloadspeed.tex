Figure~\ref{ref:PacketLossDownload} below shows the download speed measured in Mbps against the increasing rate of packet loss caused by the degradation tool, as you can see the download rate only reaches around 25\%, this is due to the connection test failing at anything higher. The tests were performed on the localhost to reduce any residual degradation that may exist already in the current network.

%Graph
\begin{center}
	\begin{tikzpicture}[ every axis plot/.append style={thick}]
		\begin{axis}[
			width=\linewidth,
			height=10cm,
			grid=major,
			xmin=0, xmax=25,
			ymin=0,
			xlabel=Packet Loss (\%),
			ylabel=Download Speed (Mbps)]
			\addplot table [mark=none, search path=csv_data, col sep=comma]{PacketLossDownload.csv};
		 \end{axis}
 	\end{tikzpicture}
\end{center}
\begin{figure}[h]
	\caption{Graph displaying Packet Loss (\%) against Download Speed (Mbps)}
	\label{ref:PacketLossDownload}
\end{figure}

My initial hypothesis for the sharp decreases in download rate when packet loss increases from 0\% to 1\% in Figure~\ref{ref:PacketLossDownload} is due to the congestion control mechanisms built into TCP. TCP as mentioned in the background section allows for multiple devices to co-exist on a network and needs to dynamically balance network resources for every client. It balances available network resources through congestion control algorithms where in this context packet loss is assumed as a sign of congestion. It adapts to signs of congestion by reducing its transfer rate. This adaptation means the transfer rate drastically reduced until it reaches a platform. The transfer rate is dictated by a few factors, one known as the congestion window and the advertised size of the receiving window (Rwnd). The overall window size is the smaller of these two numbers, and the smaller the window size, the less packets can be send in one time frame. The congestion window therefore is reduced every time packet loss is detected. 

The test devised to prove this hypothesis will involve once again an iperf client/server where a packet is dropped after a time period while the congestion window size is tracked. Below is a graph showing the result of a test spanning 10 seconds where a single incoming packet was dropped every 1 second, the graph is measuring Cwnd sizes on the y axis and Time on the x axis.

%Graph
\begin{center}
	\begin{tikzpicture}[ every axis plot/.append style={thick}]
		\begin{axis}[
			mark=square,
			width=\linewidth,
			height=10cm, 
			ymin=0, ymax=3400,
			xmin=0, xmax=9.9,
			grid=major,
			xlabel=Time (Seconds),
			ylabel=Cwnd]
			\addplot table [mark=none, search path=csv_data, col sep=comma]{PacketLossCwnd.csv};
		 \end{axis}
 	\end{tikzpicture}
\end{center}
\begin{figure}[h]
	\caption{Experimental designed to show how packet loss effects the Cwnd value}
	\label{ref:PacketLossCwndExperiment}
\end{figure}

As you can see around the 1 second mark in Figure~\ref{ref:PacketLossCwndExperiment} there are almost identical drops in the Cwnd size. These controlled drops demonstrates that packet loss causes the congestion window to reduce in size and if packet loss is substantial enough and frequent enough it can cause a huge drop in transfer rates.

This is known in live networks as the `sawtooth' pattern where the client is probing for more bandwidth. This is demonstrated in Figure~\ref{ref:SawtoothGraph}.

%Graph
\begin{center}
	\begin{tikzpicture}[every axis plot/.append style={thick}]
		\begin{axis}[
			mark=square,
			width=\linewidth,
			height=10cm, 
			ymin=0, 
			xmin=0, xmax=10,
			grid=major,
			xlabel=Time (Seconds),
			ylabel=Cwnd]
			\addplot table [mark=none, search path=csv_data, col sep=comma]{PacketLossLiveNetworkCwnd.csv};
		 \end{axis}
 	\end{tikzpicture}
\end{center}
\begin{figure}[h]
	\caption{Live Cwnd capture on a real-world connection}
	\label{ref:SawtoothGraph}
\end{figure}

From the period of 3 seconds onwards is the main operation of the TCP protocol to continuously test the network for more bandwidth. The maximum cwnd of the connection can be worked out by drawing a line above the spiked sections.

%Graph
\begin{center}
	\begin{tikzpicture}[every axis plot/.append style={thick}]
		\begin{axis}[
			title=\underline{Cwnd X Time},
			mark=square,
			width=\linewidth,
			height=10cm, 
			ymin=0, 
			xmin=0, xmax=0.5,
			grid=major,
			xlabel=Time (Seconds),
			ylabel=Cwnd]
			\addplot table [mark=none, search path=csv_data, col sep=comma]{PacketLossCwnd.csv};
		 \end{axis}
 	\end{tikzpicture}
\end{center}
\begin{figure}[h]
	\caption{Graph displaying TCP Slow Start}
	\label{ref:SlowStart}
\end{figure}

The initial increase in transfer rate in Figure~\ref{ref:PacketLossCwndExperiment}, is displayed in Figure~\ref{ref:SlowStart}. This is known as the TCP Slow Start and this is used by the protocol to naturally align itself into the network eco-system. The protocol starts off with a modest transfer rate to prevent situations where an initial abnormally large transfer rate would fill buffers and cause network congestion. It adjusts by incorporating the techniques explained above to manually increases its transfer rate to fit naturally into the current network.